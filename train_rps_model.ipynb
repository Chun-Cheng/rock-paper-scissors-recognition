{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be8e1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cheng/Projects/school/machine_learning/final/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/cheng/.cache/kagglehub/datasets/adilshamim8/rock-paper-scissors/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "path = adilshamim8_rock_paper_scissors_path = kagglehub.dataset_download('adilshamim8/rock-paper-scissors')\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21df84e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a418005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定資料路徑\n",
    "DATA_DIR = os.path.join(path, 'train', 'train')\n",
    "ANNOTATION_FILE = os.path.join(DATA_DIR, '_annotations.csv')\n",
    "\n",
    "# 自訂 Dataset 類別\n",
    "class RockPaperScissorsDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "    #     image = Image.open(img_path).convert(\"RGB\")\n",
    "    #     label_str = self.img_labels.iloc[idx, 1]\n",
    "    #     label_map = {'rock': 0, 'paper': 1, 'scissors': 2}\n",
    "    #     label = label_map[label_str]\n",
    "    #     if self.transform:\n",
    "    #         image = self.transform(image)\n",
    "    #     return image, label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path = os.path.join(self.img_dir, self.img_labels.loc[idx, 'filename'])\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            label_str = self.img_labels.loc[idx, 'class']\n",
    "            label_map = {'Rock': 0, 'Paper': 1, 'Scissors': 2}\n",
    "            label = label_map[label_str]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError in __getitem__: {e}\")\n",
    "            print(f\"Index value: {idx}\")\n",
    "            print(f\"img_labels content:\\n{self.img_labels.head()}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c33d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 圖像轉換與切分資料集\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = RockPaperScissorsDataset(ANNOTATION_FILE, DATA_DIR, transform=transform)\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "# Create indices for the dataset\n",
    "indices = np.arange(len(dataset))\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create subsets using the indices\n",
    "train_set = Subset(dataset, train_indices)\n",
    "val_set = Subset(dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fcafcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.5741\n",
      "Epoch 2/5, Loss: 0.1755\n",
      "Epoch 3/5, Loss: 0.1368\n",
      "Epoch 4/5, Loss: 0.1109\n",
      "Epoch 5/5, Loss: 0.1120\n"
     ]
    }
   ],
   "source": [
    "# 設定裝置與模型\n",
    "device = torch.device('mps') # use 'mps' for MacOS, 'cuda' for Nvidia GPU, or 'cpu'\n",
    "model = models.resnext50_32x4d(weights=models.ResNeXt50_32X4D_Weights.IMAGENET1K_V2)\n",
    "model.fc = nn.Linear(model.fc.in_features, 3)\n",
    "model = model.to(device)\n",
    "\n",
    "# 設定損失與優化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 訓練模型\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5ee64bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已儲存為 rps_resnext_model.pth\n"
     ]
    }
   ],
   "source": [
    "# 儲存模型權重\n",
    "torch.save(model.state_dict(), \"rps_resnext_model.pth\")\n",
    "print(\"模型已儲存為 rps_resnext_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
